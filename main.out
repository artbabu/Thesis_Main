\BOOKMARK [0][-]{section*.1}{Abstract}{}% 1
\BOOKMARK [0][-]{chapter*.2}{Contents}{}% 2
\BOOKMARK [0][-]{chapter*.3}{List of Tables}{}% 3
\BOOKMARK [0][-]{chapter*.4}{List of Figures}{}% 4
\BOOKMARK [0][-]{chapter.1}{Introduction}{}% 5
\BOOKMARK [1][-]{section.1.1}{Natural Language Semantics}{chapter.1}% 6
\BOOKMARK [1][-]{section.1.2}{Computational Semantics}{chapter.1}% 7
\BOOKMARK [1][-]{section.1.3}{Project Overview}{chapter.1}% 8
\BOOKMARK [0][-]{chapter.2}{Background}{}% 9
\BOOKMARK [1][-]{section.2.1}{Semantic Textual Similarity \(STS\)}{chapter.2}% 10
\BOOKMARK [2][-]{subsection.2.1.1}{STS Applications}{section.2.1}% 11
\BOOKMARK [2][-]{subsection.2.1.2}{Data for STS}{section.2.1}% 12
\BOOKMARK [1][-]{section.2.2}{Brief History of Meaning Representation}{chapter.2}% 13
\BOOKMARK [1][-]{section.2.3}{Machine Learning Models}{chapter.2}% 14
\BOOKMARK [2][-]{subsection.2.3.1}{Traditional ML Models}{section.2.3}% 15
\BOOKMARK [2][-]{subsection.2.3.2}{Neural Network}{section.2.3}% 16
\BOOKMARK [1][-]{section.2.4}{Word Representation}{chapter.2}% 17
\BOOKMARK [1][-]{section.2.5}{Sentence Representation Model}{chapter.2}% 18
\BOOKMARK [2][-]{subsection.2.5.1}{Traditional Machine Learning Models}{section.2.5}% 19
\BOOKMARK [2][-]{subsection.2.5.2}{Neural Models}{section.2.5}% 20
\BOOKMARK [1][-]{section.2.6}{Summary}{chapter.2}% 21
\BOOKMARK [0][-]{chapter.3}{State-of-the-art Methods in Semantic Similarity }{}% 22
\BOOKMARK [1][-]{section.3.1}{Ensemble Model}{chapter.3}% 23
\BOOKMARK [2][-]{subsection.3.1.1}{Features}{section.3.1}% 24
\BOOKMARK [2][-]{subsection.3.1.2}{Objective Function}{section.3.1}% 25
\BOOKMARK [1][-]{section.3.2}{Convolutional Neural Network}{chapter.3}% 26
\BOOKMARK [2][-]{subsection.3.2.1}{Sentence Model using CNN}{section.3.2}% 27
\BOOKMARK [2][-]{subsection.3.2.2}{Similarity Measure using FCNN}{section.3.2}% 28
\BOOKMARK [1][-]{section.3.3}{Recurrent Neural Network}{chapter.3}% 29
\BOOKMARK [2][-]{subsection.3.3.1}{Bi-LSTM with max pooling}{section.3.3}% 30
\BOOKMARK [1][-]{section.3.4}{Summary}{chapter.3}% 31
\BOOKMARK [0][-]{chapter.4}{Experimental Results}{}% 32
\BOOKMARK [1][-]{section.4.1}{Evaluation Metric}{chapter.4}% 33
\BOOKMARK [1][-]{section.4.2}{Experiment Setup}{chapter.4}% 34
\BOOKMARK [1][-]{section.4.3}{Encoder Architectures - Performance}{chapter.4}% 35
\BOOKMARK [2][-]{subsection.4.3.1}{Analysis}{section.4.3}% 36
\BOOKMARK [1][-]{section.4.4}{Hyper-parameter Tuning}{chapter.4}% 37
\BOOKMARK [2][-]{subsection.4.4.1}{Analysis}{section.4.4}% 38
\BOOKMARK [1][-]{section.4.5}{Input features for Traditional ML}{chapter.4}% 39
\BOOKMARK [1][-]{section.4.6}{Transfer Learning}{chapter.4}% 40
\BOOKMARK [1][-]{section.4.7}{Comparison with the Original Models}{chapter.4}% 41
\BOOKMARK [1][-]{section.4.8}{Summary}{chapter.4}% 42
\BOOKMARK [0][-]{chapter.5}{Conclusion}{}% 43
\BOOKMARK [0][-]{chapter*.32}{Bibliography}{}% 44
